{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install the OpenAI gym and Atari environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Requirement already satisfied: gym in /home/lotfi/anaconda3/lib/python2.7/site-packages (0.12.0)\n",
      "Requirement already satisfied: requests>=2.0 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from gym) (2.21.0)\n",
      "Requirement already satisfied: six in /home/lotfi/anaconda3/lib/python2.7/site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: scipy in /home/lotfi/anaconda3/lib/python2.7/site-packages (from gym) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from gym) (1.15.4)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from requests>=2.0->gym) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from requests>=2.0->gym) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from requests>=2.0->gym) (2018.11.29)\n",
      "Requirement already satisfied: future in /home/lotfi/anaconda3/lib/python2.7/site-packages (from pyglet>=1.2.0->gym) (0.17.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym gym[atari]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import Keras for our model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Flatten, Dense, Concatenate, Multiply, MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use an OpenAI gym environment.\n",
    "First, we choose and init the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0072027 ,  0.04302043, -0.01092955,  0.01145781])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we iterate for a big number of times. Each time, we do the following:\n",
    "* Render the environment\n",
    "* Get the observation, the reward and the `done` variable (it indicates whether we lost or not\n",
    "* (Optional) Sleep for 20ms, makes the animation smoother\n",
    "* (Optional) If we lost (`done == True`), reset the environment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    env.render()\n",
    "    obs, rew, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    time.sleep(0.02)\n",
    "    # Comment the two following lines to see cases of \"failure\"\n",
    "    #if done:\n",
    "    #   env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`obs` is an array of length 4, whose values are: <br/>\n",
    "`[position of cart, velocity of cart, angle of pole, rotation rate of pole]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BattleZone environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, there are 12 environments. The main difference is the observation type:\n",
    "* The \"normal\" environments: observations are the frames of the game. They are bigger and more complex to analyse, but more intuitive.\n",
    "* The RAM environments: observations are a 128-byte arrays, representing the RAM of the Atari console. Lighter to use, but we do not know what each byte represents.\n",
    "\n",
    "The environment is provided in binary so we cannot modify it. Besides, it is hard to replicate the logic behind the spawning of enemies, so we chose to use the OpenAI environments directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "battlezone_envs = ['BattleZone-v0', \n",
    "        'BattleZone-v4',\n",
    "        'BattleZoneDeterministic-v0', \n",
    "        'BattleZoneDeterministic-v4', \n",
    "        'BattleZoneNoFrameskip-v0', \n",
    "        'BattleZoneNoFrameskip-v4',\n",
    "        'BattleZone-ram-v0',\n",
    "        'BattleZone-ram-v4',\n",
    "        'BattleZone-ramDeterministic-v0', \n",
    "        'BattleZone-ramDeterministic-v4', \n",
    "        'BattleZone-ramNoFrameskip-v0',\n",
    "        'BattleZone-ramNoFrameskip-v4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_number = 0\n",
    "env = gym.make(battlezone_envs[env_number])\n",
    "\n",
    "env.reset()\n",
    "for _ in range(200):\n",
    "    env.render()\n",
    "    obs, rew, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    time.sleep(0.02)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to get the dimensions of the interesting parts of the observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fac6d1373c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(obs[3:36,74:96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fac6c8ba780>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(obs[38:178,8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gray(img):\n",
    "    # Convert images to grayscale with values between 0 and 1\n",
    "    return color.rgb2gray(img).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(img):\n",
    "    # Downsampling an image for faster computing\n",
    "    return img[::2, ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(obs):\n",
    "    radar = obs[3:36,74:96]\n",
    "    scene = obs[38:178,8:]\n",
    "    radar = to_gray(radar)\n",
    "    scene = downsample(to_gray(scene))\n",
    "    return radar, scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar, scene = preprocessing(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fac6c7766d8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(scene, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be giving the model a number of frames at a time (the time of the last projectile sent can be a determining factor, so the agent may need it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_net(n_actions):\n",
    "     # Shapes of entries\n",
    "    scene_shape = (70, 76, n_frames) # We will be using the number of frames as the number of channels \n",
    "    radar_shape = (33,22, n_frames)\n",
    "    \n",
    "    # Input layers\n",
    "    scene_inp = Input(scene_shape, name='scenes', batch_shape=(n_frames, 70, 76))\n",
    "    radar_inp = Input(radar_shape, name='radars', batch_shape=(n_frames, 33, 22))\n",
    "    actions_inp = Input((n_actions,), name='actions')\n",
    "    \n",
    "    # First convolution layers\n",
    "    conv1_1 = Conv2D(filters=8, kernel_size=(8, 8), strides=(4, 4), activation='relu', data_format=\"channels_last\")(scene_inp)\n",
    "    conv2_1 = Conv2D(filters=8, kernel_size=(4, 4), strides=(2, 2), activation='relu', data_format=\"channels_last\")(radar_inp)\n",
    "    \n",
    "    # First maxpooling attempt\n",
    "    maxpool1_1 = MaxPooling2D(pool_size=(8, 8), strides=(4, 4))(conv1_1)\n",
    "    maxpool2_1 = MaxPooling2D(pool_size=(4, 4), strides=(2, 2))(conv2_1)\n",
    "    \n",
    "    # Flattening results\n",
    "    flattened_1 = Flatten()(maxpool1_1)\n",
    "    flattened_2 = Flatten()(maxpool2_1)\n",
    "    \n",
    "    # Concatenating the results of the two entries\n",
    "    concat = Concatenate(axis=1)([flattened_1, flattened_2])\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(n_actions, activation='relu')(concat)\n",
    "    \n",
    "    # Multiply by the actions\n",
    "    filtered_output = Multiply()([output, actions_inp])\n",
    "    \n",
    "    # Creating the model\n",
    "    model = Model(input=[scene_inp, radar_inp, actions_inp], \n",
    "                  output=filtered_output,\n",
    "                 )\n",
    "    \n",
    "    # Optimizer choice and tuning\n",
    "    rms = RMSprop(lr = 0.00025)\n",
    "    \n",
    "    # Compiling\n",
    "    model.compile(optimizer=rms, loss='mse')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nor_env_net(n_actions):\n",
    "    \n",
    "    # Shapes of entries\n",
    "    scene_shape = (70, 76, n_frames) # We will be using the number of frames as the number of channels \n",
    "    radar_shape = (33,22, n_frames)\n",
    "    \n",
    "    # Input layers\n",
    "    scene_inp = Input(scene_shape, name='scenes', batch_shape=(4, 70, 76))\n",
    "    radar_inp = Input(radar_shape, name='radars', batch_shape=(4, 33, 22))\n",
    "    actions_inp = Input((n_actions,), name='actions')\n",
    "    \n",
    "    # First convolution layers\n",
    "    conv1_1 = Conv2D(filters=16, kernel_size=(8, 8), strides=(4, 4), activation='relu')(scene_inp)\n",
    "    conv2_1 = Conv2D(filters=16, kernel_size=(4, 4), strides=(2, 2), activation='relu')(radar_inp)\n",
    "    \n",
    "    # First maxpooling attempt\n",
    "    maxpool1_1 = MaxPooling2D(pool_size=(8, 8), strides=(4, 4))(conv1_1)\n",
    "    maxpool2_1 = MaxPooling2D(pool_size=(4, 4), strides=(2, 2))(conv2_1)\n",
    "    \n",
    "    # Second convolutional layers\n",
    "    conv1_2 = Conv2D(filters=32, kernel_size=(4, 4), strides=(2, 2), activation='relu')(maxpool1_1)\n",
    "    conv2_2 = Conv2D(filters=32, kernel_size=(2, 2), strides=(1, 1), activation='relu')(maxpool2_1)\n",
    "    \n",
    "    # Second maxpooling attempt\n",
    "    maxpool1_2 = MaxPooling2D(pool_size=(4, 4), strides=(2, 2))(conv1_2)\n",
    "    maxpool2_2 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(conv2_2)\n",
    "    \n",
    "    # Flattening results\n",
    "    flattened_1 = Flatten('channels_first')(maxpool1_2)\n",
    "    flattened_2 = Flatten('channels_first')(maxpool2_2)\n",
    "    \n",
    "    # Concatenating the results of the two entries\n",
    "    concat = Concatenate(axis=1)([flattened_1, flattened_2])\n",
    "    \n",
    "    # Applying a fully-connected layer\n",
    "    hidden_layer = Dense(256, activation='relu')(concat)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(n_actions, activation='relu')(hidden_layer)\n",
    "    \n",
    "    # Multiply by the actions\n",
    "    filtered_output = Multiply()([output, actions_inp])\n",
    "    \n",
    "    # Creating the model\n",
    "    model = Model(input=[scene_inp, radar_inp, actions_inp], \n",
    "                  output=filtered_output,\n",
    "                 )\n",
    "    \n",
    "    # Optimizer choice and tuning\n",
    "    rms = RMSprop(lr = 0.00025)\n",
    "    \n",
    "    # Compiling\n",
    "    model.compile(optimizer=rms, loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lotfi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"mu...)`\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-4ac676ab4125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m plot_model(model,\n\u001b[1;32m      4\u001b[0m            \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model.png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m            show_shapes=True)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "model = nor_env_net(4)\n",
    "plot_model(model,\n",
    "           to_file='model.png',\n",
    "           show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = list()\n",
    "radars = list()\n",
    "radar, scene = preprocessing(obs)\n",
    "\n",
    "for i in range(n_frames):\n",
    "    radars.append(radar)\n",
    "    scenes.append(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 33, 22)\n",
      "(4, 70, 76)\n"
     ]
    }
   ],
   "source": [
    "s1 = (len(radars), radar.shape[0], radar.shape[1])\n",
    "s2 = (len(scenes), scene.shape[0], scene.shape[1])\n",
    "print(s1)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(radars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306,\n",
       "       0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306,\n",
       "       0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306,\n",
       "       0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306,\n",
       "       0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306,\n",
       "       0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306,\n",
       "       0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306,\n",
       "       0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306,\n",
       "       0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306,\n",
       "       0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306,\n",
       "       0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306,\n",
       "       0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306, 0.2806306,\n",
       "       0.2806306, 0.2806306, 0.2806306, 0.2806306], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenes[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for i in range(s1[0]):\n",
    "    for j in range(s1[1]):\n",
    "        for k in range(s1[2]):\n",
    "            radars_array[i,j,k] = radars[i][j,k]\n",
    "            \n",
    "for i in range(s2[0]):\n",
    "    for j in range(s2[1]):\n",
    "        for k in range(s2[2]):\n",
    "            scenes_array[i,j,k] = scenes[i][j,k]\n",
    "\"\"\"\n",
    "radars = np.reshape(radars, (len(radars), radar.shape[0], radar.shape[1]))\n",
    "scenes = np.reshape(scenes, (len(scenes), scene.shape[0], scene.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv2d_9: expected ndim=4, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-04afbace07ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-7743e4704bb9>\u001b[0m in \u001b[0;36msample_net\u001b[0;34m(n_actions)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# First convolution layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mconv1_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"channels_last\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mconv2_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"channels_last\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mradar_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv2d_9: expected ndim=4, found ndim=2"
     ]
    }
   ],
   "source": [
    "model = sample_net(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected scenes to have 4 dimensions, but got array with shape (4, 33, 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-c7a094982012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mradars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected scenes to have 4 dimensions, but got array with shape (4, 33, 22)"
     ]
    }
   ],
   "source": [
    "l = model.predict([radars, scenes, np.ones((4,1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model being already defined, we will now implement a function that will run a fitting iteration: We will be using a Deep Q-learning approach, improving our network everytime the agent makes an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_iteration(model, gamma, start_scenes, start_radars, actions, rewards, next_scenes, next_radars, is_terminal):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    - model: The DQN defined in the last cell\n",
    "    - gamma: \"Discount factor\" (should be 0.99)\n",
    "    - start_scenes & start_radars: starting states\n",
    "    - actions: array of one-hot encoded actions corresponding to the start states\n",
    "    - rewards: array of rewards corresponding to the start states and actions\n",
    "    - next_scenes & next_radars: the resulting states corresponding to the start states and actions\n",
    "    - is_terminal: boolean array of whether the resulting state is terminal\n",
    "    \n",
    "    \"\"\"\n",
    "    # First, we predict the Q values of the next states. We pass ones as the \"mask\".\n",
    "    next_Q_values = model.predict([next_scenes, next_radars, np.ones(actions.shape)])\n",
    "    \n",
    "    # The Q value a terminal state is 0 by definition\n",
    "    next_Q_values[is_terminal] = 0\n",
    "    \n",
    "    # We apply the formula of the Q value\n",
    "    Q_values = rewards + gamma * np.max(next_Q_values, axis=1)\n",
    "    \n",
    "    # Fit the model.\n",
    "    model.fit([start_scenes, start_radars, actions], actions * Q_values[:, None], nb_epoch=1, batch_size=len(start_scenes), verbose=0)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a buffer (a ringed buffer specifically) in order to put a limit to the memory usage (and prevent any saturation or malfunctioning of our agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RingBuf:\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        # We allocate one extra element, so that self.start == self.end always means the buffer is empty\n",
    "        self.data = [None] * (size + 1) # Size will indicate the size of memory we want to allocate\n",
    "        self.start = 0\n",
    "        self.end = 0\n",
    "        \n",
    "    def append(self, element):\n",
    "        self.data[self.end] = element\n",
    "        self.end = (self.end + 1) % len(self.data)\n",
    "        # end == start means the buffer has one too many element. \n",
    "        # We then remove the first element by incrementing start.\n",
    "        if (self.end == self.start):\n",
    "            self.start = (self.start + 1) % len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[(self.start + index) % len(self.data)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if (self.end < self.start):\n",
    "            return (self.end + len(self.data) - self.start)\n",
    "        else:\n",
    "            return (self.end - self.start)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class doubleEdgedQueue(list):\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        \n",
    "    def add(self,el):\n",
    "        if len(self) >= size:\n",
    "            self.remove(0)\n",
    "        self.append(el)\n",
    "        \n",
    "    def get_size(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_from_iter(i):\n",
    "    if (i > 1e6):\n",
    "        return 0.1\n",
    "    else:\n",
    "        return (1 - 1e-6 * 0.9 * i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(model, observations):\n",
    "    # Choose the best action according to the model's prediction\n",
    "    \n",
    "    #Initializing\n",
    "    scenes = list()\n",
    "    radars = list()\n",
    "    \n",
    "    # Preprocessing the observations\n",
    "    for obs in observations:\n",
    "        radar, scene = preprocessing(obs)\n",
    "        radars.append(radar)\n",
    "        scenes.append(scenes)\n",
    "        \n",
    "    # Reshaping for the model to use (typically 4 frames of each type, as the model has 4 input channels)\n",
    "    radars = np.reshape(radars, (len(radars), radar.shape[0], radars.shape[1]))\n",
    "    scenes = np.reshape(scenes, (len(scenes), scenes.shape[0], scenes.shape[1]))\n",
    "    \n",
    "    action = model.predict([scenes, radars, actions])\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(n_games, model):\n",
    "    \n",
    "    i = 0\n",
    "    rew_max = 0\n",
    "    \n",
    "    for _ in n_games:\n",
    "        \n",
    "        rew_total = 0\n",
    "        env.reset()\n",
    "        action = 0\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        observations = doubleEdgedQueue(n_frames)\n",
    "        \n",
    "        while not done:\n",
    "            env.render()\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            observations.add(obs)\n",
    "            \n",
    "            if np.random.uniform() > epsilon_from_iter(i):\n",
    "                action = choose_action(model, observations)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "                \n",
    "            model = fit_iteration(model, gamma, start_scenes, start_radars, actions, rewards, next_scenes, next_radars, is_terminal)\n",
    "\n",
    "            i += 1\n",
    "            rew_total += rew\n",
    "            \n",
    "        rew_max = max(rew_max, rew_total)\n",
    "        env.close()\n",
    "    \n",
    "    return rew_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAM environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_number = 6\n",
    "env = gym.make(battlezone_envs[env_number])\n",
    "\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    obs, rew, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    time.sleep(0.02)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
