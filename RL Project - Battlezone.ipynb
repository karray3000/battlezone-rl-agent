{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install the OpenAI gym and Atari environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Requirement already satisfied: gym in /home/lotfi/anaconda3/lib/python2.7/site-packages (0.12.0)\n",
      "Requirement already satisfied: requests>=2.0 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from gym) (2.21.0)\n",
      "Requirement already satisfied: six in /home/lotfi/anaconda3/lib/python2.7/site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: scipy in /home/lotfi/anaconda3/lib/python2.7/site-packages (from gym) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from gym) (1.15.4)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from requests>=2.0->gym) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from requests>=2.0->gym) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lotfi/anaconda3/lib/python2.7/site-packages (from requests>=2.0->gym) (2018.11.29)\n",
      "Requirement already satisfied: future in /home/lotfi/anaconda3/lib/python2.7/site-packages (from pyglet>=1.2.0->gym) (0.17.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym gym[atari]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import Keras for our model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Flatten, Dense, Concatenate, Multiply\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use an OpenAI gym environment.\n",
    "First, we choose and init the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0072027 ,  0.04302043, -0.01092955,  0.01145781])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we iterate for a big number of times. Each time, we do the following:\n",
    "* Render the environment\n",
    "* Get the observation, the reward and the `done` variable (it indicates whether we lost or not\n",
    "* (Optional) Sleep for 20ms, makes the animation smoother\n",
    "* (Optional) If we lost (`done == True`), reset the environment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    env.render()\n",
    "    obs, rew, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    time.sleep(0.02)\n",
    "    # Comment the two following lines to see cases of \"failure\"\n",
    "    #if done:\n",
    "    #   env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`obs` is an array of length 4, whose values are: <br/>\n",
    "`[position of cart, velocity of cart, angle of pole, rotation rate of pole]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BattleZone environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, there are 12 environments. The main difference is the observation type:\n",
    "* The \"normal\" environments: observations are the frames of the game. They are bigger and more complex to analyse, but more intuitive.\n",
    "* The RAM environments: observations are a 128-byte arrays, representing the RAM of the Atari console. Lighter to use, but we do not know what each byte represents.\n",
    "\n",
    "The environment is provided in binary so we cannot modify it. Besides, it is hard to replicate the logic behind the spawning of enemies, so we chose to use the OpenAI environments directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "battlezone_envs = ['BattleZone-v0', \n",
    "        'BattleZone-v4',\n",
    "        'BattleZoneDeterministic-v0', \n",
    "        'BattleZoneDeterministic-v4', \n",
    "        'BattleZoneNoFrameskip-v0', \n",
    "        'BattleZoneNoFrameskip-v4',\n",
    "        'BattleZone-ram-v0',\n",
    "        'BattleZone-ram-v4',\n",
    "        'BattleZone-ramDeterministic-v0', \n",
    "        'BattleZone-ramDeterministic-v4', \n",
    "        'BattleZone-ramNoFrameskip-v0',\n",
    "        'BattleZone-ramNoFrameskip-v4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_number = 0\n",
    "env = gym.make(battlezone_envs[env_number])\n",
    "\n",
    "env.reset()\n",
    "for _ in range(200):\n",
    "    env.render()\n",
    "    obs, rew, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    time.sleep(0.02)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to get the dimensions of the interesting parts of the observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f88cc9d80f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(obs[3:36,74:96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f88cc958be0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(obs[38:178,8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gray(img):\n",
    "    # Convert images to grayscale with values between 0 and 1\n",
    "    return color.rgb2gray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(img):\n",
    "    # Downsampling an image for faster computing\n",
    "    return img[::2, ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(obs):\n",
    "    radar = obs[3:36,74:96]\n",
    "    scene = obs[38:178,8:]\n",
    "    radar = to_gray(radar)\n",
    "    scene = downsample(to_gray(scene))\n",
    "    return radar, scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar, scene = preprocessing(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nor_env_net(n_actions):\n",
    "    \n",
    "    # Shapes of entries\n",
    "    scene_shape = (70, 76, 4) # 4 is the number of channels\n",
    "    radar_shape = (33,22, 4)\n",
    "    \n",
    "    # Input layers\n",
    "    scene_inp = Input(scene_shape, name='scenes')\n",
    "    radar_inp = Input(radar_shape, name='radars')\n",
    "    actions_inp = Input((n_actions,), name='actions')\n",
    "    \n",
    "    # Convolution layers\n",
    "    conv1_1 = Conv2D(filters=16, kernel_size=(8, 8), strides=(4, 4), activation='relu')(scene_inp)\n",
    "    conv2_1 = Conv2D(filters=16, kernel_size=(4, 4), strides=(2, 2), activation='relu')(radar_inp)\n",
    "    \n",
    "    conv1_2 = Conv2D(filters=32, kernel_size=(4, 4), strides=(2, 2), activation='relu')(conv1_1)\n",
    "    conv2_2 = Conv2D(filters=32, kernel_size=(2, 2), strides=(1, 1), activation='relu')(conv2_1)\n",
    "    \n",
    "    # Flattening results\n",
    "    flattened_1 = Flatten('channels_last')(conv1_2)\n",
    "    flattened_2 = Flatten('channels_last')(conv2_2)\n",
    "    \n",
    "    # Concatenating the results of the two entries\n",
    "    concat = Concatenate(axis=1)([flattened_1, flattened_2])\n",
    "    \n",
    "    # Applying a fully-connected layer\n",
    "    hidden_layer = Dense(256, activation='relu')(concat)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(n_actions, activation='relu')(hidden_layer)\n",
    "    \n",
    "    # Multiply by the actions\n",
    "    filtered_output = Multiply()([output, actions_inp])\n",
    "    \n",
    "    # Creating the model\n",
    "    model = Model(input=[scene_inp, radar_inp, actions_inp], \n",
    "                  output=filtered_output,\n",
    "                 )\n",
    "    \n",
    "    # Optimizer choice and tuning\n",
    "    rms = RMSprop(lr = 0.00025)\n",
    "    \n",
    "    # Compiling\n",
    "    model.compile(optimizer=rms, loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elyes/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"mu...)`\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "model = nor_env_net(4)\n",
    "plot_model(model,\n",
    "           to_file='model.png',\n",
    "           show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model being already defined, we will now implement a function that will run a fitting iteration: We will be using a Deep Q-learning approach, improving our network everytime the agent makes an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_iteration(model, gamma, start_scenes, start_radars, actions, rewards, next_scenes, next_radars, is_terminal):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    - model: The DQN defined in the last cell\n",
    "    - gamma: \"Discount factor\" (should be 0.99)\n",
    "    - start_scenes & start_radars: starting states\n",
    "    - actions: array of one-hot encoded actions corresponding to the start states\n",
    "    - rewards: array of rewards corresponding to the start states and actions\n",
    "    - next_scenes & next_radars: the resulting states corresponding to the start states and actions\n",
    "    - is_terminal: boolean array of whether the resulting state is terminal\n",
    "    \n",
    "    \"\"\"\n",
    "    # First, we predict the Q values of the next states. We pass ones as the \"mask\".\n",
    "    next_Q_values = model.predict([next_scenes, next_radars, np.ones(actions.shape)])\n",
    "    \n",
    "    # The Q value a terminal state is 0 by definition\n",
    "    next_Q_values[is_terminal] = 0\n",
    "    \n",
    "    # We apply the formula of the Q value\n",
    "    Q_values = rewards + gamma * np.max(next_Q_values, axis=1)\n",
    "    \n",
    "    # Fit the model.\n",
    "    model.fit([start_scenes, start_radars, actions], actions * Q_values[:, None], nb_epoch=1, batch_size=len(start_scenes), verbose=0)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a buffer (a ringed buffer specifically) in order to put a limit to the memory usage (and prevent any saturation or malfunctioning of our agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RingBuf:\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        # We allocate one extra element, so that self.start == self.end always means the buffer is empty\n",
    "        self.data = [None] * (size + 1) # Size will indicate the size of memory we want to allocate\n",
    "        self.start = 0\n",
    "        self.end = 0\n",
    "        \n",
    "    def append(self, element):\n",
    "        self.data[self.end] = element\n",
    "        self.end = (self.end + 1) % len(self.data)\n",
    "        # end == start means the buffer has one too many element. \n",
    "        # We then remove the first element by incrementing start.\n",
    "        if (self.end == self.start):\n",
    "            self.start = (self.start + 1) % len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[(self.start + index) % len(self.data)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if (self.end < self.start):\n",
    "            return (self.end + len(self.data) - self.start)\n",
    "        else:\n",
    "            return (self.end - self.start)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_from_iter(i):\n",
    "    if (i > 1e6):\n",
    "        return 0.1\n",
    "    else:\n",
    "        return (1 - 1e-6 * 0.9 * i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(model, obs):\n",
    "    radar, scene = preprocessing(obs)\n",
    "    \n",
    "    action = model.predict([scene, radar, actions])\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(n_games, model):\n",
    "    \n",
    "    i = 0\n",
    "    rew_max = 0\n",
    "    \n",
    "    for _ in n_games:\n",
    "        \n",
    "        rew_total = 0\n",
    "        env.reset()\n",
    "        action = 0\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            env.render()\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            \n",
    "            if np.random.uniform() > epsilon_from_iter(i):\n",
    "                action = choose_action(model, obs)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "                \n",
    "            model = fit_iteration(model, gamma, start_scenes, start_radars, actions, rewards, next_scenes, next_radars, is_terminal)\n",
    "\n",
    "            i += 1\n",
    "            rew_total += rew\n",
    "            \n",
    "        rew_max = max(rew_max, rew_total)\n",
    "        env.close()\n",
    "    \n",
    "    return rew_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAM environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_number = 6\n",
    "env = gym.make(battlezone_envs[env_number])\n",
    "\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    obs, rew, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    time.sleep(0.02)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
