{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install the OpenAI gym and Atari environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/elyes/anaconda3/lib/python3.6/site-packages (0.11.0)\n",
      "Requirement already satisfied: scipy in /home/elyes/anaconda3/lib/python3.6/site-packages (from gym) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/elyes/anaconda3/lib/python3.6/site-packages (from gym) (1.15.4)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /home/elyes/anaconda3/lib/python3.6/site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: six in /home/elyes/anaconda3/lib/python3.6/site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: requests>=2.0 in /home/elyes/anaconda3/lib/python3.6/site-packages (from gym) (2.21.0)\n",
      "Requirement already satisfied: future in /home/elyes/anaconda3/lib/python3.6/site-packages (from pyglet>=1.2.0->gym) (0.17.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/elyes/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/elyes/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/elyes/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/elyes/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym) (2018.11.29)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym gym[atari]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import Keras for our model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Flatten, Dense, Concatenate, Multiply, MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use an OpenAI gym environment.\n",
    "First, we choose and init the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02809307, 0.02559229, 0.03536608, 0.03344944])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we iterate for a big number of times. Each time, we do the following:\n",
    "* Render the environment\n",
    "* Get the observation, the reward and the `done` variable (it indicates whether we lost or not\n",
    "* (Optional) Sleep for 20ms, makes the animation smoother\n",
    "* (Optional) If we lost (`done == True`), reset the environment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    env.render()\n",
    "    obs, rew, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    time.sleep(0.02)\n",
    "    # Comment the two following lines to see cases of \"failure\"\n",
    "    #if done:\n",
    "    #   env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`obs` is an array of length 4, whose values are: <br/>\n",
    "`[position of cart, velocity of cart, angle of pole, rotation rate of pole]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BattleZone environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, there are 12 environments. The main difference is the observation type:\n",
    "* The \"normal\" environments: observations are the frames of the game. They are bigger and more complex to analyse, but more intuitive.\n",
    "* The RAM environments: observations are a 128-byte arrays, representing the RAM of the Atari console. Lighter to use, but we do not know what each byte represents.\n",
    "\n",
    "The environment is provided in binary so we cannot modify it. Besides, it is hard to replicate the logic behind the spawning of enemies, so we chose to use the OpenAI environments directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "battlezone_envs = ['BattleZone-v0', \n",
    "        'BattleZone-v4',\n",
    "        'BattleZoneDeterministic-v0', \n",
    "        'BattleZoneDeterministic-v4', \n",
    "        'BattleZoneNoFrameskip-v0', \n",
    "        'BattleZoneNoFrameskip-v4',\n",
    "        'BattleZone-ram-v0',\n",
    "        'BattleZone-ram-v4',\n",
    "        'BattleZone-ramDeterministic-v0', \n",
    "        'BattleZone-ramDeterministic-v4', \n",
    "        'BattleZone-ramNoFrameskip-v0',\n",
    "        'BattleZone-ramNoFrameskip-v4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_number = 0\n",
    "env = gym.make(battlezone_envs[env_number])\n",
    "\n",
    "env.reset()\n",
    "for _ in range(2000):\n",
    "    env.render()\n",
    "    obs, rew, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    time.sleep(0.02)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to get the dimensions of the interesting parts of the observation:\n",
    "* The radar part (that is in the top middle part of the frame)\n",
    "* The view from the tank's cockpit, restrained to the green field (and a little of the background mountains) which will contain the moving enemies as well as the projectiles' range and most importantly the cursor indicating what is directly in front of our tank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faa409de438>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALYAAAD8CAYAAADaM14OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAACwhJREFUeJzt3V+oHPUZxvHv01RbqYKm0YPEWK0NlLRoChIs9cIqllQKUSjBQCUXUr1QiOBNyI1SKFio2lyIEGswBWsa/FNzEdqGIGhvRJNKjEmLNhhMOOYYVExBlJi3FzOHrmn27J6Z2Zk97z4fOOzu7OzOm995Msz+ds47igjMsvlK1wWYjYKDbSk52JaSg20pOdiWkoNtKTnYlpKDbSk52JbSV+u8WNJqYDOwCPh9RDw0YP2J+ZrzkhWXtLatmYMzrW1rDJyIiIsHrVQ52JIWAY8BNwNHgdck7YyIg1XfM5N129e1tq3NV29ubVtj4MgwK9U5FFkFvBMRhyPic2A7sKbG+5k1pk6wlwLv9Tw+Wi4z61ytY+xhSLoLuGvU2zHrVSfYx4BlPY8vK5d9SURsAbbAZH14tG7VORR5DVgu6UpJ5wK3AzubKcusnsp77Ig4Jele4K8U031bI+KtxiobIxv2b5j3a9qcqRj3+rpQ6xg7InYBuxqqxawx/ubRUnKwLSUH21JysC0ltdl+YRzmsT2DUFjA47A3Iq4dtJL32JaSg20pOdiWkoNtKTnYlpKDbSmlnO6rMpUFYzOd1akFMHae7rPJ5WBbSg62peRgW0oOtqW0oGdFFvCJPAvSmIy3Z0VscjnYlpKDbSk52JaSg20pOdiWUt3G7+8CJ4EvgFPDTMOYtaGJbqs/jogTDbyPWWN8KGIp1Q12AH+TtLfsg202FuoeilwfEcckXQLslvTPiHi5dwU3frcu1NpjR8Sx8nYGeIHiujRnrrMlIq71B0trU+VgS/qGpAtm7wM/AQ40VZhZHXUORaaAFyTNvs8fI+IvjVTVY64zyqqcOdb0+2WTZbzrXNHgMHBNg7WYNcbTfZaSg20pOdiWkoNtKTnYlpKDbSk52JaSg20pOdiWkoNtKTnYltLYtDjrd7JMmycmjUMNbRmHf2vFGtzizCaXg20pOdiWkoNtKTnYlpKDbSk52JaSg20pOdiWkoNtKTnYlpKDbSkNbJgjaSvwM2AmIr5fLlsM/Am4AngXWBsRH42uzHZkPNmpn+z/1mH22E8Bq89YthHYExHLgT3lY7OxMTDYZVvgD89YvAbYVt7fBtzacF1mtVQ9xp6KiOny/vsUDSrNxkbta9BERMz1BwRu/G5dqLrHPi7pUoDydqbfim78bl2oGuydwPry/nrgxWbKMWvGMNN9zwA3AEskHQUeAB4Cdki6EzgCrB1lkZPixIn+VxVcsmRJi5UsfAODHRHr+jx1U8O1mDXG3zxaSg62peRgW0oOtqXUaieoqe9NxbrtZ/8smv2kHBvegEvouROUTS4H21JysC0lB9tScrAtJQfbUnKwLSUH21JysC0lB9tScrAtJQfbUnKwLSUH21JysC0lB9tScrAtJQfbUnKwLaWBwZa0VdKMpAM9yx6UdEzSG+XPLaMt02x+qjZ+B3g0IlaWP7uaLcusnqqN383GWp1j7Hsl7S8PVS7qt5KkuyS9Lun1Tz/6tMbmzIZXNdiPA1cBK4Fp4OF+K/b2xz7vovMqbs5sfioFOyKOR8QXEXEaeAJY1WxZZvVUCvbs1QxKtwEH+q1r1oWqjd9vkLQSCIrrPN49whrTmavBez9u/D4/VRu/PzmCWswa428eLSUH21JysC0lB9tSqn1lXmuOZz6a4z22peRgW0oOtqXkYFtKDral5GBbSg62peRgW0oOtqXkYFtKDral5GBbSg62peRgW0oOtqXkYFtKDral5GBbSg62pTRM4/dlkl6SdFDSW5I2lMsXS9ot6e3ytm/HVbO2DbPHPgXcHxErgOuAeyStADYCeyJiObCnfGw2FoZp/D4dEfvK+yeBQ8BSYA2wrVxtG3DrqIo0m695HWNLugL4AfAqMBUR0+VT7wNTfV7jxu/WuqGDLel84Dngvoj4pPe5iAiKzqv/x43frQtDBVvSORShfjoini8XH5/tk13ezoymRLP5G2ZWRBRtgw9FxCM9T+0E1pf31wMvNl+eWTUqjiLmWEG6HngFeBM4XS7eRHGcvQO4HDgCrI2IOa8uJqnvxjbs33DW5Zuv3jxnffM1V9P1tlqM9ath0lqcVfyd742Iawe99zCN3/8OqM/TNw16vVkX/M2jpeRgW0oOtqXkYFtKE9f4fdJmHiaV99iWkoNtKTnYlpKDbSk52JaSg20pTdx0X1vG4WSrSeY9tqXkYFtKDral5GBbSg62peRgW0oOtqXkYFtKDral5GBbSg62peRgW0rDdIJaBvyBoptqAFsiYrOkB4FfAh+Uq26KiF0D3mvujZ1Fv25B0HyXqCb5JKjCCH5/zXSC4n+N3/dJugDYK2l3+dyjEfHbKtWZjdIwLc6mgeny/klJs43fzcZWncbvAPdK2i9pa79r0PQ2fq9Vqdk81Gn8/jhwFbCSYo/+8Nle19v4vYF6zYZSufF7RByPiC8i4jTwBLBqdGWazU/lxu+zVzMo3QYcaL48s2qGmRX5EXAH8KakN8plm4B1klZSTAG+C9w9kgrNKqjT+H3OOWuzLvmbR0vJwbaUHGxLycG2lAaeBNXoxiqcBDWXti6hV8WknQTV4u9iqJOgvMe2lBxsS8nBtpQcbEvJwbaUHGxLKWXj94X6d5Ljbq5xHTfeY1tKDral5GBbSg62peRgW0oOtqW0oM/u66fqtFSTU4EL9ey+cRi7AXx2n00uB9tScrAtJQfbUnKwLaVhGr9/HXgZ+BrFSVPPRsQDkq4EtgPfBPYCd0TE5wPeq70pmD6qfOqf6xN/v9mPcZ75gObHoUWNzYp8BtwYEddQdFZdLek64DcUjd+/A3wE3FmnWrMmDQx2FP5TPjyn/AngRuDZcvk24NaRVGhWwbBthBeVDSlngN3Av4GPI+JUucpR+lzlwI3frQtDBbvsg70SuIyiD/Z3h92AG79bF+Y1KxIRHwMvAT8ELpQ0+xc4lwHHGq7NrLJhGr9fLOnC8v55wM3AIYqA/7xcbT3w4qiKNJuvYab7rqb4cLiI4j/Cjoj4laRvU0z3LQb+AfwiIj4b8F6dT/dVMe5TY+NeX8Oauc5jROynuFLYmcsP4+vO2JjyN4+WkoNtKTnYlpKDbSml7ATVtConQbF/RMWcxQKe4RgZ77EtJQfbUnKwLSUH21JysC0lB9tSarsT1AfAkfLhEqB/u6TJ4XEoDDsO34qIiwet1Gqwv7Rh6XX/8YHHYVbT4+BDEUvJwbaUugz2lg63PU48DoVGx6GzY2yzUfKhiKXUSbAlrZb0L0nvSNrYRQ1dkLRV0oykAz3LFkvaLent8vaiLmscNUnLJL0k6aCktyRtKJc3Og6tB1vSIuAx4KfACmCdpBVt19GRp4DVZyzbCOyJiOXAnvJxZqeA+yNiBXAdcE/5+290HLrYY68C3omIw2UTy+3Amg7qaF1EvAx8eMbiNRRdAGACWsVFxHRE7Cvvn6Ro5bGUhsehi2AvBd7redy3PdqEmIqI6fL++8BUl8W0SdIVFB0QXqXhcfCHxzESxRTVRExTSTofeA64LyI+6X2uiXHoItjHgGU9jye9PdpxSZcClLczHdczcpLOoQj10xHxfLm40XHoItivAcslXSnpXOB2YGcHdYyLnRQt4mACWsVJEvAkcCgiHul5qtFx6OQLGkm3AL+jaJu2NSJ+3XoRHZD0DHADxZlsx4EHgD8DO4DLKc58XBsRZ37ATEPS9cArwJvA6XLxJorj7MbGwd88Wkr+8GgpOdiWkoNtKTnYlpKDbSk52JaSg20pOdiW0n8BBYmG4SyYHv4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(obs[3:36,74:96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faa4095f7f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(obs[80:178,8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gray(img):\n",
    "    # Convert images to grayscale with values between 0 and 1\n",
    "    return color.rgb2gray(img).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(img):\n",
    "    # Downsampling an image for faster computing\n",
    "    return img[::2, ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(obs):\n",
    "    radar = obs[3:36,74:96]\n",
    "    scene = obs[80:178,8:]\n",
    "    radar = to_gray(radar)\n",
    "    scene = downsample(to_gray(scene))\n",
    "    return radar, scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar, scene = preprocessing(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faa408c9128>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(scene, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 22) (49, 76)\n"
     ]
    }
   ],
   "source": [
    "print(radar.shape, scene.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be giving the model a number of frames at a time (the time of the last projectile sent can be a determining factor, so the agent may need it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 2 #Number of input frames per input\n",
    "n_actions = 18 #Dimension of action space\n",
    "batch_size = 32 #Number of entries (states+actions) to feed the NN\n",
    "memory_size = 100000 # Memory size for experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a Huber loss function for our model to prevent heavy penalization of high errors (linear instead of quadratic). We will implement the function so it can be used within a Keras neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def huber_loss(a, b):\n",
    "    e = a - b\n",
    "    quad = e**2 / 2\n",
    "    lin = abs(e) - 1/2\n",
    "    use_lin = (abs(e) > 1.0)\n",
    "    use_lin = K.cast(use_lin, 'float32')\n",
    "    return use_lin * lin + ( 1 - use_lin) * quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_net():\n",
    "     # Shapes of entries\n",
    "    scene_shape = (49, 76, n_frames) # We will be using the number of frames as the number of channels \n",
    "    radar_shape = (33,22, n_frames)\n",
    "    \n",
    "    # Input layers\n",
    "    scene_inp = Input(shape=scene_shape, name='scenes')#, batch_shape=(n_frames, 49, 76))\n",
    "    radar_inp = Input(shape=radar_shape, name='radars')#, batch_shape=(n_frames, 33, 22))\n",
    "    actions_inp = Input((n_actions,), name='actions')\n",
    "    \n",
    "    # First convolution layers\n",
    "    conv1_1 = Conv2D(filters=8, kernel_size=(8, 8), strides=(4, 4), activation='relu', data_format=\"channels_last\")(scene_inp)\n",
    "    conv2_1 = Conv2D(filters=8, kernel_size=(4, 4), strides=(2, 2), activation='relu', data_format=\"channels_last\")(radar_inp)\n",
    "    \n",
    "    # First maxpooling attempt\n",
    "    maxpool1_1 = MaxPooling2D(pool_size=(8, 8), strides=(4, 4))(conv1_1)\n",
    "    maxpool2_1 = MaxPooling2D(pool_size=(4, 4), strides=(2, 2))(conv2_1)\n",
    "    \n",
    "    # Flattening results\n",
    "    flattened_1 = Flatten()(maxpool1_1)\n",
    "    flattened_2 = Flatten()(maxpool2_1)\n",
    "    \n",
    "    # Concatenating the results of the two entries\n",
    "    concat = Concatenate(axis=1)([flattened_1, flattened_2])\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(n_actions, activation='softmax')(concat)\n",
    "    \n",
    "    # Multiply by the actions\n",
    "    filtered_output = Multiply()([output, actions_inp])\n",
    "    \n",
    "    # Creating the model\n",
    "    model = Model(input=[scene_inp, radar_inp, actions_inp], \n",
    "                  output=filtered_output,\n",
    "                 )\n",
    "    \n",
    "    # Optimizer choice and tuning\n",
    "    rms = RMSprop(lr = 0.00025)\n",
    "    \n",
    "    # Compiling\n",
    "    model.compile(optimizer=rms, loss=huber_loss)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nor_env_net():\n",
    "    \n",
    "    # Shapes of entries\n",
    "    scene_shape = (49, 76, n_frames) # We will be using the number of frames as the number of channels \n",
    "    radar_shape = (33,22, n_frames)\n",
    "    \n",
    "    # Input layers\n",
    "    scene_inp = Input(scene_shape, name='scenes')#, batch_shape=(4, 49, 76))\n",
    "    radar_inp = Input(radar_shape, name='radars')#, batch_shape=(4, 33, 22))\n",
    "    actions_inp = Input((n_actions,), name='actions')\n",
    "    \n",
    "    # First convolution layers\n",
    "    conv1_1 = Conv2D(filters=16, kernel_size=(8, 8), strides=(4, 4), activation='relu')(scene_inp)\n",
    "    conv2_1 = Conv2D(filters=16, kernel_size=(4, 4), strides=(2, 2), activation='relu')(radar_inp)\n",
    "    \n",
    "    # First maxpooling attempt\n",
    "    # maxpool1_1 = MaxPooling2D(pool_size=(8, 8), strides=(4, 4))(conv1_1)\n",
    "    # maxpool2_1 = MaxPooling2D(pool_size=(4, 4), strides=(2, 2))(conv2_1)\n",
    "    \n",
    "    # Second convolutional layers\n",
    "    conv1_2 = Conv2D(filters=32, kernel_size=(4, 4), strides=(2, 2), activation='relu')(conv1_1)\n",
    "    conv2_2 = Conv2D(filters=32, kernel_size=(2, 2), strides=(1, 1), activation='relu')(conv2_1)\n",
    "    \n",
    "    # Second maxpooling attempt\n",
    "    maxpool1_2 = MaxPooling2D(pool_size=(4, 4), strides=(2, 2))(conv1_2)\n",
    "    maxpool2_2 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(conv2_2)\n",
    "    \n",
    "    # Flattening results\n",
    "    flattened_1 = Flatten('channels_first')(maxpool1_2)\n",
    "    flattened_2 = Flatten('channels_first')(maxpool2_2)\n",
    "    \n",
    "    # Concatenating the results of the two entries\n",
    "    concat = Concatenate(axis=1)([flattened_1, flattened_2])\n",
    "    \n",
    "    # Applying a fully-connected layer\n",
    "    hidden_layer = Dense(256, activation='relu')(concat)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(n_actions, activation='softmax')(hidden_layer)\n",
    "    \n",
    "    # Multiply by the actions\n",
    "    filtered_output = Multiply()([output, actions_inp])\n",
    "    \n",
    "    # Creating the model\n",
    "    model = Model(input=[scene_inp, radar_inp, actions_inp], \n",
    "                  output=filtered_output,\n",
    "                 )\n",
    "    \n",
    "    # Optimizer choice and tuning\n",
    "    rms = RMSprop(lr = 0.00025)\n",
    "    \n",
    "    # Compiling\n",
    "    model.compile(optimizer=rms, loss=huber_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_exclusive():\n",
    "    \n",
    "    # Shape of entries\n",
    "    radar_shape = (n_frames, 33*22,) # Since we will be using a fully connected network here,\n",
    "                                    # we will provide a flattened version of the radar frame\n",
    "    \n",
    "    # Input layers\n",
    "    radar_inp = Input(shape=radar_shape, name='radars')#, batch_shape=(n_frames, 33, 22))\n",
    "    actions_inp = Input((n_actions,), name='actions')\n",
    "    \n",
    "    # Hidden layers\n",
    "    hid1 = Dense(512, activation=\"relu\")(radar_inp)\n",
    "    hid2 = Dense(256, activation=\"relu\")(hid1)\n",
    "    \n",
    "    # Output\n",
    "    output = Dense(n_actions, activation='softmax')(hid2)\n",
    "    \n",
    "    # Multiply by the actions\n",
    "    filtered_output = Multiply()([output, actions_inp])\n",
    "    \n",
    "    # Creating the model\n",
    "    model = Model(input=[radar_inp, actions_inp], \n",
    "                  output=filtered_output,\n",
    "                 )\n",
    "    \n",
    "    # Optimizer choice and tuning\n",
    "    rms = RMSprop(lr = 0.00025)\n",
    "    \n",
    "    # Compiling\n",
    "    model.compile(optimizer=rms, loss=huber_loss)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "#model = nor_env_net()\n",
    "#model = sample_net()\n",
    "#model= radar_exclusive()\n",
    "plot_model(model,\n",
    "           to_file='model.png',\n",
    "           show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = list()\n",
    "radars = list()\n",
    "radar, scene = preprocessing(obs)\n",
    "\n",
    "for i in range(n_frames):\n",
    "    radars.append(radar)\n",
    "    scenes.append(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 33, 22)\n",
      "(2, 49, 76)\n"
     ]
    }
   ],
   "source": [
    "s1 = (len(radars), radar.shape[0], radar.shape[1])\n",
    "s2 = (len(scenes), scene.shape[0], scene.shape[1])\n",
    "print(s1)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.        , 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412, 0.43529412, 0.43529412, 0.43529412, 0.43529412,\n",
       "       0.43529412], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenes[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for i in range(s1[0]):\n",
    "    for j in range(s1[1]):\n",
    "        for k in range(s1[2]):\n",
    "            radars_array[i,j,k] = radars[i][j,k]\n",
    "            \n",
    "for i in range(s2[0]):\n",
    "    for j in range(s2[1]):\n",
    "        for k in range(s2[2]):\n",
    "            scenes_array[i,j,k] = scenes[i][j,k]\n",
    "\"\"\"\n",
    "radars = np.reshape(radars, (radar.shape[0], radar.shape[1], n_frames))\n",
    "scenes = np.reshape(scenes, (scene.shape[0], scene.shape[1], n_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 22, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lotfi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"mu...)`\n"
     ]
    }
   ],
   "source": [
    "model = radar_exclusive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 726)\n"
     ]
    }
   ],
   "source": [
    "flattened_radars = np.reshape(radars, (n_frames, 33*22))\n",
    "print(flattened_radars.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rads = list()\n",
    "acts = [np.ones((1, n_actions))] * batch_size\n",
    "\n",
    "for i in range(batch_size):\n",
    "    rads.append(flattened_radars)\n",
    "    \n",
    "rads = np.reshape(rads, (batch_size, n_frames, 33*22))\n",
    "acts = np.reshape(acts, (batch_size, n_actions))\n",
    "\n",
    "inp = [rads, acts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.predict(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 2, 18)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model being already defined, we will now implement a function that will run a fitting iteration: We will be using a Deep Q-learning approach, improving our network everytime the agent makes an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a buffer (a ringed buffer specifically) in order to put a limit to the memory usage (and prevent any saturation or malfunctioning of our agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RingBuf:\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        # We allocate one extra element, so that self.start == self.end always means the buffer is empty\n",
    "        self.data = [None] * (size + 1) # Size will indicate the size of memory we want to allocate\n",
    "        self.start = 0\n",
    "        self.end = 0\n",
    "        \n",
    "    def append(self, element):\n",
    "        self.data[self.end] = element\n",
    "        self.end = (self.end + 1) % len(self.data)\n",
    "        # end == start means the buffer has one too many element. \n",
    "        # We then remove the first element by incrementing start.\n",
    "        if (self.end == self.start):\n",
    "            self.start = (self.start + 1) % len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[(self.start + index) % len(self.data)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if (self.end < self.start):\n",
    "            return (self.end + len(self.data) - self.start)\n",
    "        else:\n",
    "            return (self.end - self.start)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]\n",
    "    \n",
    "    def get_sample(self, size):\n",
    "        l = list()\n",
    "        for _ in range(size):\n",
    "            i = random.randint(0, len(self))\n",
    "            l.append(self.data[i])\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_from_iter(i):\n",
    "    if (i > 1e6):\n",
    "        return 0.1\n",
    "    else:\n",
    "        return (1 - 1e-6 * 0.9 * i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_iteration(model, target_model, gamma, start_scenes, start_radars, actions, rewards, next_scenes, next_radars, is_terminal):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    - model: The DQN defined in the last cell\n",
    "    - target_model: the target to stabilize the converging of our network\n",
    "    - gamma: \"Discount factor\" (should be 0.99)\n",
    "    - start_scenes & start_radars: starting states\n",
    "    - actions: array of one-hot encoded actions corresponding to the start states\n",
    "    - rewards: array of rewards corresponding to the start states and actions\n",
    "    - next_scenes & next_radars: the resulting states corresponding to the start states and actions\n",
    "    - is_terminal: boolean array of whether the resulting state is terminal\n",
    "    \n",
    "    \"\"\"\n",
    "    # First, we predict the Q values of the next states. We pass ones as the \"mask\".\n",
    "    next_Q_values = target_model.predict([next_scenes, next_radars, np.ones((1, n_actions))])\n",
    "    \n",
    "    # The Q value a terminal state is 0 by definition\n",
    "    next_Q_values[is_terminal] = 0\n",
    "    \n",
    "    # We apply the formula of the Q value\n",
    "    Q_values = rewards + gamma * np.max(next_Q_values, axis=1)\n",
    "    \n",
    "    # Fit the model.\n",
    "    model.fit([start_scenes, start_radars, actions], actions * Q_values[:, None], nb_epoch=1, batch_size=len(start_scenes), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(model, stacked_radars, stacked_scenes):\n",
    "    # Choose the best action according to the model's prediction\n",
    "    \n",
    "    #Initializing\n",
    "    radars = list(stacked_radars)\n",
    "    scenes = list(stacked_scenes)\n",
    "\n",
    "        \n",
    "    # Reshaping for the model to use (typically 4 frames of each type, as the model has 4 input channels)\n",
    "    radars = np.reshape(radars, (batch_size, radars[0].shape[0], radars[0].shape[1], n_frames))\n",
    "    scenes = np.reshape(scenes, (batch_size, scenes[0].shape[0], scenes[0].shape[1], n_frames))\n",
    "    \n",
    "    prediction = model.predict([scenes, radars, np.ones((1,n_actions))])\n",
    "    \n",
    "    return np.argmax(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stacked_radars, stacked_scenes, observation):\n",
    "    radar, scene = preprocessing(observation)\n",
    "    if (len(stacked_radars) < n_frames):\n",
    "        for _ in range(n_frames):\n",
    "            stacked_radars.append(radar)\n",
    "            stacked_scenes.append(scene)\n",
    "    else:\n",
    "        stacked_radars.append(radar)\n",
    "        stacked_scenes.append(scene)\n",
    "    return stacked_radars, stacked_scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model(model):\n",
    "    model.save('tmp_model')\n",
    "    return load_model('tmp_model', custom_objects={'huber_loss': huber_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(n_games, model, memory):\n",
    "    \n",
    "    gamma = 0.99\n",
    "    i = 0\n",
    "    rew_max = 0\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        \n",
    "        rew_total = 0\n",
    "        env.reset()\n",
    "        action = 0\n",
    "        \n",
    "        done = False\n",
    "        is_terminal = [False] * n_actions\n",
    "        \n",
    "        stacked_scenes = deque([], maxlen=n_frames)\n",
    "        stacked_radars = deque([], maxlen=n_frames)\n",
    "        \n",
    "        previous_scenes = list()\n",
    "        previous_radars = list()\n",
    "        \n",
    "        while not done :\n",
    "            env.render()\n",
    "            observation, rew, done, info = env.step(action)\n",
    "                        \n",
    "            previous_radars, previous_scenes = deepcopy(stacked_radars), deepcopy(stacked_scenes)\n",
    "            \n",
    "            stacked_radars, stacked_scenes = stack_frames(stacked_radars,\n",
    "                                                          stacked_scenes,\n",
    "                                                          observation)\n",
    "            \n",
    "            is_terminal = np.reshape(is_terminal, (1, n_actions))\n",
    "            \n",
    "            if rew_total == 0:\n",
    "                previous_radars, previous_scenes = deepcopy(stacked_radars), deepcopy(stacked_scenes)\n",
    "            \n",
    "            if np.random.uniform() > epsilon_from_iter(i) and len(stacked_radars) == n_frames:\n",
    "                action = choose_action(model, stacked_radars, stacked_scenes)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            \n",
    "            if done:\n",
    "                is_terminal[0,action] = True\n",
    "            \n",
    "            # Actions one-hot vector\n",
    "            actions = np.zeros((1,n_actions))\n",
    "            actions[0,action] = 1\n",
    "            \n",
    "            # Reshaping for the model\n",
    "            start_scenes = np.reshape(list(previous_scenes), (batch_size, previous_scenes[0].shape[0], previous_scenes[0].shape[1], n_frames))\n",
    "            start_radars = np.reshape(list(previous_radars), (batch_size, previous_radars[0].shape[0], previous_radars[0].shape[1], n_frames))\n",
    "            next_scenes = np.reshape(list(stacked_scenes), (batch_size, stacked_scenes[0].shape[0], stacked_scenes[0].shape[1], n_frames))\n",
    "            next_radars = np.reshape(list(stacked_radars), (batch_size, stacked_radars[0].shape[0], stacked_radars[0].shape[1], n_frames))\n",
    "            \n",
    "            # Stabilizing technique\n",
    "            target_model = model\n",
    "            if ((i % 10000) == 0):\n",
    "                target_model = copy_model(model)\n",
    "            \n",
    "            # Fitting iteration\n",
    "            memory.append([start_scenes, start_radars, action, rew, next_scenes, next_radars, done])\n",
    "            sample = memory.get_sample(batch_size)\n",
    "            \n",
    "            fit_iteration(model, target_model, gamma, start_scenes, start_radars, actions, rew, next_scenes, next_radars, is_terminal)\n",
    "\n",
    "            i += 1\n",
    "            rew_total += rew\n",
    "            \n",
    "        rew_max = max(rew_max, rew_total)\n",
    "        env.close()\n",
    "    \n",
    "    return rew_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_iteration_radar(model, target_model, gamma, sample):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    - model: The DQN defined in the last cell\n",
    "    - target_model: the target to stabilize the converging of our network\n",
    "    - gamma: \"Discount factor\" (should be 0.99)\n",
    "    - start_scenes & start_radars: starting states\n",
    "    - actions: array of one-hot encoded actions corresponding to the start states\n",
    "    - rewards: array of rewards corresponding to the start states and actions\n",
    "    - next_scenes & next_radars: the resulting states corresponding to the start states and actions\n",
    "    - is_terminal: boolean array of whether the resulting state is terminal\n",
    "    \n",
    "    \"\"\"\n",
    "    start_radars, actions, rewards, next_radars, is_terminal = list(), list(), list(), list(), list()\n",
    "    for i in range(len(sample)):\n",
    "        start_radars.append(sample[i][0])\n",
    "        actions.append(sample[i][1])\n",
    "        rewards.append(sample[i][2])\n",
    "        next_radars.append(sample[i][3])\n",
    "        is_terminal.append(sample[i][4])\n",
    "        \n",
    "    start_radars = np.reshape(start_radars, ((len(start_radars),) + start_radars[0].shape))\n",
    "    actions = np.reshape(actions, (len(actions), actions[0].shape[-1]))\n",
    "    next_radars = np.reshape(next_radars, ((len(next_radars),) + next_radars[0].shape))\n",
    "    is_terminal = np.reshape(is_terminal, ((1, len(is_terminal))))\n",
    "    # First, we predict the Q values of the next states. We pass ones as the \"mask\".\n",
    "    mask = np.array([np.ones((n_actions,))] * next_radars.shape[0])\n",
    "    next_Q_values = target_model.predict([next_radars, mask])[:,1,:]\n",
    "    next_Q_values = np.reshape(next_Q_values, (next_Q_values.shape[0], 1 , next_Q_values.shape[1]))\n",
    "    \n",
    "    # The Q value a terminal state is 0 by definition\n",
    "    is_terminal = np.transpose(is_terminal)\n",
    "    print(is_terminal.shape)\n",
    "    print(next_Q_values.shape)\n",
    "    next_Q_values[is_terminal] = 0\n",
    "    \n",
    "    # We apply the formula of the Q value\n",
    "    Q_values = rewards + gamma * np.argmax(np.max(next_Q_values, axis=1), axis=1)\n",
    "    \n",
    "    # Fit the model.\n",
    "    model.fit([start_radars, actions], actions * Q_values[:, None], nb_epoch=1, batch_size=batch_size, verbose=0)\n",
    "\n",
    "def choose_action_radar(model, stacked_radars):\n",
    "    # Choose the best action according to the model's prediction\n",
    "    \n",
    "    #Initializing\n",
    "    radars = list(stacked_radars)\n",
    "\n",
    "    # Reshaping for the model to use\n",
    "    radars = np.reshape(radars, (batch_size, n_frames, 22*33))\n",
    "    \n",
    "    prediction = model.predict([radars, np.ones((1,n_actions))])\n",
    "    \n",
    "    return np.argmax(prediction[0][-1])\n",
    "\n",
    "def stack_radars(stacked_radars, observation):\n",
    "    radar, scene = preprocessing(observation)\n",
    "    if (len(stacked_radars) < n_frames):\n",
    "        for _ in range(n_frames):\n",
    "            stacked_radars.append(radar)\n",
    "    else:\n",
    "        stacked_radars.append(radar)\n",
    "    return stacked_radars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game_rad(n_games, model, memory):\n",
    "    \n",
    "    gamma = 0.99\n",
    "    i = 0\n",
    "    rew_max = 0\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        \n",
    "        rew_total = 0\n",
    "        env.reset()\n",
    "        action = 0\n",
    "        \n",
    "        done = False\n",
    "        is_terminal = [False] * n_actions\n",
    "        \n",
    "        stacked_radars = deque([], maxlen=n_frames)\n",
    "        \n",
    "        previous_radars = list()\n",
    "        \n",
    "        while not done :\n",
    "            env.render()\n",
    "            observation, rew, done, info = env.step(action)\n",
    "                        \n",
    "            previous_radars = deepcopy(stacked_radars)\n",
    "            \n",
    "            stacked_radars = stack_radars(stacked_radars, observation)\n",
    "            \n",
    "            is_terminal = np.reshape(is_terminal, (1, n_actions))\n",
    "            \n",
    "            if rew_total == 0:\n",
    "                previous_radars = deepcopy(stacked_radars)\n",
    "            \n",
    "            if np.random.uniform() > epsilon_from_iter(i) and len(stacked_radars) == n_frames:\n",
    "                action = choose_action_radar(model, stacked_radars)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            \n",
    "            if done:\n",
    "                is_terminal[0,action] = True\n",
    "            \n",
    "            # Actions one-hot vector\n",
    "            actions = np.zeros((1,n_actions))\n",
    "            actions[0,action] = 1\n",
    "            \n",
    "            # Reshaping for the model\n",
    "            start_radars = np.reshape(list(previous_radars),\n",
    "                                      (n_frames, previous_radars[0].shape[0]*previous_radars[0].shape[1]))\n",
    "            next_radars = np.reshape(list(stacked_radars),\n",
    "                                     (n_frames, stacked_radars[0].shape[0]*stacked_radars[0].shape[1]))\n",
    "            \n",
    "            # Stabilizing technique\n",
    "            target_model = model\n",
    "            if ((i % 10000) == 0):\n",
    "                target_model = copy_model(model)\n",
    "            \n",
    "            # Fitting iteration\n",
    "            memory.append([start_radars, actions, rew, next_radars, done])\n",
    "            if (i > 100):\n",
    "                sample = memory.get_sample(batch_size)\n",
    "\n",
    "                fit_iteration_radar(model, target_model, gamma, sample)\n",
    "\n",
    "            i += 1\n",
    "            rew_total += rew\n",
    "            \n",
    "        rew_max = max(rew_max, rew_total)\n",
    "        env.close()\n",
    "    \n",
    "    return rew_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1)\n",
      "(32, 1, 18)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (32,) (32,18) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-8ff549aabfab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRingBuf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_game_rad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-97-329754ab139f>\u001b[0m in \u001b[0;36mrun_game_rad\u001b[0;34m(n_games, model, memory)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mfit_iteration_radar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-83f86455ab7c>\u001b[0m in \u001b[0;36mfit_iteration_radar\u001b[0;34m(model, target_model, gamma, sample)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# We apply the formula of the Q value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mQ_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_Q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Fit the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32,) (32,18) "
     ]
    }
   ],
   "source": [
    "memory = RingBuf(memory_size)\n",
    "run_game_rad(10, model, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAM environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_number = 6\n",
    "env = gym.make(battlezone_envs[env_number])\n",
    "\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    obs, rew, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    time.sleep(0.02)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
